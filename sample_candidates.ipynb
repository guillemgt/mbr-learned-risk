{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FRzlslxcYFP"
      },
      "source": [
        "# Candidate sampling (for all of train/valid/test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKGGl4pKcdJO"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YmkSEL-Q8jm"
      },
      "outputs": [],
      "source": [
        "DO_TRAIN = True\n",
        "DO_VALID = True\n",
        "DO_TEST = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DCW2Ut2wwAs"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PM91CukQnid"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    FULL_DATA_DIR = f'/content/drive/My Drive/mbr-reranking/{DATA_DIR}'\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    FULL_DATA_DIR = DATA_DIR\n",
        "\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU2EkLf3tld_"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvDnCthDRjA7"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!pip install sentencepiece\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtQayoyFR1Pf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDai3hwAclc_"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOUrA3m-HMNb"
      },
      "outputs": [],
      "source": [
        "PRETRAINED_MODEL = \"mbart\"\n",
        "USE_COLLATE_FN = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SFHXAYjDESl"
      },
      "outputs": [],
      "source": [
        "if PRETRAINED_MODEL == \"mbart\":\n",
        "    from transformers import MBartForConditionalGeneration, MBartTokenizer\n",
        "    tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"de_DE\")\n",
        "    model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\").to(device)\n",
        "    model = model.eval()\n",
        "else:\n",
        "    from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoConfig\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "    model.config.max_length = 512\n",
        "    model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5u57Yyh1aIR"
      },
      "outputs": [],
      "source": [
        "def get_sentence_pairs(split=\"train\", subsample_factor=1):\n",
        "    sentences_src = []\n",
        "    sentences_dst = []\n",
        "    with open(f\"{FULL_DATA_DIR}/{split}.deu\", 'r', encoding='utf-8') as fp:\n",
        "        for line in fp:\n",
        "            sentences_src.append(line.strip())\n",
        "\n",
        "    with open(f\"{FULL_DATA_DIR}/{split}.eng\", 'r', encoding='utf-8') as fp:\n",
        "        for line in fp:\n",
        "            sentences_dst.append(line.strip())\n",
        "\n",
        "    if len(sentences_src) == len(sentences_dst):\n",
        "        print(len(sentences_src), f\"{split} pairs\")\n",
        "    else:\n",
        "        print(f\"WARNING: different number of {split} sentences: {len(sentences_src)} vs {len(sentences_dst)}\")\n",
        "\n",
        "    num_samples = len(sentences_src) // subsample_factor\n",
        "    return sentences_src[:num_samples], sentences_dst[:num_samples]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cvi_-hG88YX"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Sampler, DataLoader\n",
        "import math\n",
        "\n",
        "class LengthBatchSampler(Sampler):\n",
        "    def __init__(self, sorted_data_with_indices, batch_size):\n",
        "        self.sorted_data_with_indices = [a for (a,b) in sorted_data_with_indices]\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch = []\n",
        "        i = 0\n",
        "        while i < len(self.sorted_data_with_indices):\n",
        "            yield self.sorted_data_with_indices[i:i+self.batch_size]\n",
        "            i += self.batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.sorted_data_with_indices) / self.batch_size)\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class SimpleDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "if PRETRAINED_MODEL == \"mbart\":\n",
        "    def collator(batch):\n",
        "        # Unzip the batch to separate indices and sentences\n",
        "        indices, sentences = zip(*batch)\n",
        "\n",
        "        # Tokenize the sentences. This automatically handles padding\n",
        "        tokenized_inputs = tokenizer(list(sentences), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Return a dict containing the tokenized inputs and the original indices\n",
        "        return {\"indices\": indices, \"input_ids\": tokenized_inputs['input_ids'], \"attention_mask\": tokenized_inputs['attention_mask']}\n",
        "else:\n",
        "    def collator(batch):\n",
        "        # Unzip the batch to separate indices and sentences\n",
        "        indices, sentences = zip(*batch)\n",
        "        sentences = [\"translate English to German: \" + sentence for sentence in sentences]\n",
        "\n",
        "        # Tokenize the sentences. This automatically handles padding\n",
        "        tokenized_inputs = tokenizer(list(sentences), padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Return a dict containing the tokenized inputs and the original indices\n",
        "        return {\"indices\": indices, \"input_ids\": tokenized_inputs['input_ids'], \"attention_mask\": tokenized_inputs['attention_mask']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFDdy_c6cvQV"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v22UYBMQYtTP"
      },
      "outputs": [],
      "source": [
        "def get_dataset(split=\"train\", batch_size=1, subsample_factor=1):\n",
        "    sentences_src, sentences_dst = get_sentence_pairs(split=split, subsample_factor=subsample_factor)\n",
        "    sorted_sentences_with_indices = sorted(enumerate(sentences_src), key=lambda x: len(x[1]))\n",
        "\n",
        "    # Create the dataset and sampler\n",
        "    dataset = SimpleDataset(sorted_sentences_with_indices)\n",
        "    # sampler = LengthBatchSampler(dataset, batch_size=batch_size)\n",
        "\n",
        "    # Create the DataLoader\n",
        "    # if USE_COLLATE_FN:\n",
        "    #     data_loader = DataLoader(dataset, batch_sampler=sampler, collate_fn=collator)\n",
        "    # else:\n",
        "    #     data_loader = DataLoader(dataset, batch_sampler=sampler)\n",
        "\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return sentences_src, data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6O0waQ1czsN"
      },
      "source": [
        "## Sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XT4UIRxu2DRU"
      },
      "outputs": [],
      "source": [
        "def sample_sentences(data_loader, num_sentences, num_samples=1, num_beams=1, do_sample=True, max_length=384, temperature=1.0):\n",
        "    sampled_sentences = [\"\" for sentence in range(num_sentences) for c in range(num_samples)]\n",
        "    torch.manual_seed(0)\n",
        "    with torch.cuda.amp.autocast(enabled=True):\n",
        "        with torch.no_grad():\n",
        "            if USE_COLLATE_FN:\n",
        "                for step, batch in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    original_indices = batch['indices']\n",
        "\n",
        "                    if PRETRAINED_MODEL == \"mbart\":\n",
        "                        translated_tokens = model.generate(input_ids=input_ids, attention_mask=attention_mask, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"], do_sample=do_sample, num_return_sequences=num_samples, num_beams=num_beams, max_length=max_length, temperature=temperature).detach().cpu()\n",
        "                        decoded = tokenizer.batch_decode(translated_tokens, skip_special_tokens=False)\n",
        "                        decoded = [sentence.split('</s>')[0] for sentence in decoded]\n",
        "                    else:\n",
        "                        translated_tokens = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=do_sample, num_return_sequences=num_samples, num_beams=num_beams, max_length=max_length, temperature=temperature).detach().cpu()\n",
        "                        decoded = tokenizer.batch_decode(translated_tokens, skip_special_tokens=False)\n",
        "\n",
        "                    for index, sentence in zip(indices, decoded):\n",
        "                        sampled_sentences[index] = sentence\n",
        "\n",
        "                    torch.cuda.empty_cache()\n",
        "            else:\n",
        "                for step, (indices, sentences) in tqdm(enumerate(data_loader), total=len(data_loader)):\n",
        "\n",
        "                    if PRETRAINED_MODEL == \"mbart\":\n",
        "                        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "                        translated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"], do_sample=do_sample, num_return_sequences=num_samples, num_beams=num_beams, eos_token_id=tokenizer.eos_token_id, max_length=max_length, temperature=temperature).detach().cpu()\n",
        "                        decoded = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
        "                    else:\n",
        "                        sentences = [\"translate English to German: \" + sentence for sentence in sentences]\n",
        "                        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "                        translated_tokens = model.generate(**inputs, do_sample=do_sample, num_return_sequences=num_samples, num_beams=1, eos_token_id=tokenizer.eos_token_id, max_length=max_length, temperature=temperature).detach().cpu()\n",
        "                        decoded = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
        "\n",
        "                    for j, index in enumerate(indices):\n",
        "                        sampled_sentences[num_samples*int(index):num_samples*(int(index)+1)] = decoded[num_samples*j:num_samples*(j+1)]\n",
        "\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "    return sampled_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElRR5MoqaKtv"
      },
      "source": [
        "Train:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2kQeNQBUMOs"
      },
      "outputs": [],
      "source": [
        "if DO_TRAIN:\n",
        "    SAMPLES_PER_SENTENCE = 2\n",
        "    BATCH_SIZE = 128 // SAMPLES_PER_SENTENCE\n",
        "    sentences_src, data_loader = get_dataset(\"train\", batch_size=BATCH_SIZE, subsample_factor=1) # subsample_factor=SAMPLES_PER_SENTENCE\n",
        "    sampled_sentences = sample_sentences(data_loader, num_sentences=len(sentences_src), num_samples=SAMPLES_PER_SENTENCE)\n",
        "    with open(f'{FULL_DATA_DIR}/sampled/train.eng', 'w', encoding='utf-8') as file:\n",
        "        for sentence in sampled_sentences:\n",
        "            file.write(sentence + '\\n')\n",
        "    with open(f'{FULL_DATA_DIR}/sampled/train.info.json', 'w', encoding='utf-8') as file:\n",
        "        json.dump({\n",
        "            \"samples_per_sentence\": SAMPLES_PER_SENTENCE\n",
        "        }, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og8l4LG1UapC"
      },
      "source": [
        "Validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFiQc_Z_i4yu"
      },
      "outputs": [],
      "source": [
        "if DO_VALID:\n",
        "\n",
        "    SAMPLES_PER_SENTENCE = 30\n",
        "    BATCH_SIZE = 4\n",
        "    sentences_src, data_loader = get_dataset(\"dev\", batch_size=BATCH_SIZE)\n",
        "    sampled_sentences = sample_sentences(data_loader, num_sentences=len(sentences_src), num_samples=SAMPLES_PER_SENTENCE)\n",
        "    with open(f'{FULL_DATA_DIR}/sampled/dev.eng', 'w', encoding='utf-8') as file:\n",
        "        for sentence in sampled_sentences:\n",
        "            file.write(sentence + '\\n')\n",
        "    sampled_sentences = sample_sentences(data_loader, num_sentences=len(sentences_src), num_samples=SAMPLES_PER_SENTENCE, temperature=0.7)\n",
        "    with open(f'{FULL_DATA_DIR}/sampled/dev-cold.eng', 'w', encoding='utf-8') as file:\n",
        "        for sentence in sampled_sentences:\n",
        "            file.write(sentence + '\\n')\n",
        "\n",
        "    SAMPLES_PER_SENTENCE = 1\n",
        "    BATCH_SIZE = 16\n",
        "    sentences_src, data_loader = get_dataset(\"dev\", batch_size=BATCH_SIZE)\n",
        "    sampled_sentences = sample_sentences(data_loader, num_sentences=len(sentences_src), num_samples=SAMPLES_PER_SENTENCE, num_beams=10, do_sample=False)\n",
        "    with open(f'{FULL_DATA_DIR}/beams/dev.eng', 'w', encoding='utf-8') as file:\n",
        "        for sentence in sampled_sentences:\n",
        "            file.write(sentence + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xahj4r5TdEBr"
      },
      "source": [
        "Test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GnxIMyfupUp"
      },
      "outputs": [],
      "source": [
        "if DO_TEST:\n",
        "    SAMPLES_PER_SENTENCE = 30\n",
        "    BATCH_SIZE = 4\n",
        "    sentences_src, data_loader = get_dataset(\"test\", batch_size=BATCH_SIZE)\n",
        "    sampled_sentences = sample_sentences(data_loader, num_sentences=len(sentences_src), num_samples=SAMPLES_PER_SENTENCE)\n",
        "    with open(f'{FULL_DATA_DIR}/sampled/test.eng', 'w', encoding='utf-8') as file:\n",
        "        for sentence in sampled_sentences:\n",
        "            file.write(sentence + '\\n')\n",
        "    sampled_sentences = sample_sentences(data_loader, num_sentences=len(sentences_src), num_samples=SAMPLES_PER_SENTENCE, temperature=0.7)\n",
        "    with open(f'{FULL_DATA_DIR}/sampled/test-cold.eng', 'w', encoding='utf-8') as file:\n",
        "        for sentence in sampled_sentences:\n",
        "            file.write(sentence + '\\n')\n",
        "\n",
        "    SAMPLES_PER_SENTENCE = 1\n",
        "    BATCH_SIZE = 16\n",
        "    sentences_src, data_loader = get_dataset(\"test\", batch_size=BATCH_SIZE)\n",
        "    sampled_sentences = sample_sentences(data_loader, num_sentences=len(sentences_src), num_samples=SAMPLES_PER_SENTENCE, num_beams=10, do_sample=False)\n",
        "    with open(f'{FULL_DATA_DIR}/beams/test.eng', 'w', encoding='utf-8') as file:\n",
        "        for sentence in sampled_sentences:\n",
        "            file.write(sentence + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsvFp8Blc4FC"
      },
      "source": [
        "## End\n",
        "\n",
        "If in Google Colab, kill the session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3mE0oHDU9AA"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:\n",
        "    import time\n",
        "    time.sleep(15)\n",
        "\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWpF5fb6e3rT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
