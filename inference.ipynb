{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Inference and evaluation"
      ],
      "metadata": {
        "id": "MpbrxE91e0wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up"
      ],
      "metadata": {
        "id": "aNGzuQQJe3cP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWsBwJuWTek2"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = \"data\"\n",
        "MODEL_DIR = \"models\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    FULL_DATA_DIR = f'/content/drive/My Drive/mbr-reranking/{DATA_DIR}'\n",
        "    FULL_MODEL_DIR = f'/content/drive/My Drive/mbr-reranking/{MODEL_DIR}'\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    FULL_DATA_DIR = DATA_DIR\n",
        "    FULL_MODEL_DIR = MODEL_DIR\n",
        "\n",
        "    IN_COLAB = False"
      ],
      "metadata": {
        "id": "8W_Ppso-UGn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import sentencepiece\n",
        "except:\n",
        "    !pip install sentencepiece\n",
        "    import sentencepiece\n",
        "\n",
        "try:\n",
        "    import evaluate\n",
        "except:\n",
        "    !pip install evaluate\n",
        "    import evaluate\n",
        "\n",
        "!pip install git+https://github.com/google-research/bleurt.git\n",
        "!pip install unbabel-comet"
      ],
      "metadata": {
        "id": "XYyosBnI2Ed4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import evaluate\n",
        "bleurt = evaluate.load('bleurt')\n",
        "comet_metric = evaluate.load('comet')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "# smoothie = SmoothingFunction().method1\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "o_qFxxqK2Gs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "class Timer:\n",
        "    def __enter__(self):\n",
        "        self.start_time = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_value, traceback):\n",
        "        self.end_time = time.time()\n",
        "        elapsed_time = self.end_time - self.start_time\n",
        "        print(f\"Elapsed time: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "fR7fve1frMLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "ebrrRNvTe731"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"model_path\": \"model.pt\",\n",
        "    \"model_has_original_first\": True,\n",
        "\n",
        "    \"valid_batch_size\": 128,\n",
        "    \"compute_mbr_consensus\": True,\n",
        "}"
      ],
      "metadata": {
        "id": "RyJ0zKq436yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean pooling\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        # Mean pooling\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        mean_pooled = sum_embeddings / sum_mask\n",
        "        return mean_pooled\n",
        "\n",
        "# Model\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, pretrained_model):\n",
        "        super().__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.regression_head = torch.nn.Linear(pretrained_model.config.hidden_size, 1)\n",
        "        self.pooling = MeanPooling()\n",
        "        self.pretrained_frozen = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        if self.pretrained_frozen:\n",
        "            with torch.no_grad():\n",
        "                token_embeddings = self.pretrained_model(input_ids, attention_mask=attention_mask)\n",
        "                pooled_embedding = self.pooling(token_embeddings.last_hidden_state, attention_mask)\n",
        "        else:\n",
        "            token_embeddings = self.pretrained_model(input_ids, attention_mask=attention_mask)\n",
        "            pooled_embedding = self.pooling(token_embeddings.last_hidden_state, attention_mask)\n",
        "        return self.regression_head(pooled_embedding)\n",
        "\n",
        "    def freeze_pretrained(self):\n",
        "        self.pretrained_frozen = True\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_pretrained(self):\n",
        "        self.pretrained_frozen = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = True"
      ],
      "metadata": {
        "id": "uVQPrzBE2O6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "pretrained_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "model = RegressionModel(pretrained_model)\n",
        "model.load_state_dict(torch.load(f'{FULL_MODEL_DIR}/{config[\"model_path\"]}', map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ajn30qr8W5Fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reranker inference"
      ],
      "metadata": {
        "id": "LsvpEywtfCzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Sampler\n",
        "import random\n",
        "import math\n",
        "\n",
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, original, generated):\n",
        "        mult = len(generated) // len(original)\n",
        "        if config[\"model_has_original_first\"]:\n",
        "            texts = [original[i // mult] + tokenizer.sep_token + generated[i] for i in range(len(generated))]\n",
        "        else:\n",
        "            texts = [generated[i] + tokenizer.sep_token + original[i // mult] for i in range(len(generated))]\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True)\n",
        "\n",
        "        self.sorted_indices = sorted(range(len(texts)), key=lambda i: sum(self.encodings[\"attention_mask\"][i]))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = self.sorted_indices[index]\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['index'] = idx\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sorted_indices)"
      ],
      "metadata": {
        "id": "koJLI7wf3dDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_batch(input_ids, attention_mask):\n",
        "    # Find the maximum sequence length in this batch\n",
        "    max_len = attention_mask.sum(dim=1).max().item()\n",
        "\n",
        "    # Truncate input_ids and attention_mask to max_len\n",
        "    truncated_input_ids = input_ids[:, :max_len]\n",
        "    truncated_attention_mask = attention_mask[:, :max_len]\n",
        "\n",
        "    return truncated_input_ids, truncated_attention_mask"
      ],
      "metadata": {
        "id": "xUt2N0Wh3qaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_reranker_scores(original, generated):\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    dataset = RegressionDataset(original, generated)\n",
        "    dataloader = DataLoader(dataset, batch_size=config[\"valid_batch_size\"])\n",
        "\n",
        "    scores_flattened = [-np.inf for i in range(len(generated))]\n",
        "    with torch.no_grad(), torch.autocast(device_type=device, enabled=True):\n",
        "        pbar = tqdm(dataloader, total=len(dataloader))\n",
        "        for step, batch in enumerate(pbar):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            indices = batch['index']\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask).squeeze()\n",
        "\n",
        "            for idx, score in zip(indices, outputs):\n",
        "                scores_flattened[idx] = score.cpu().item()\n",
        "\n",
        "    return scores_flattened"
      ],
      "metadata": {
        "id": "Knwqit-q30Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "Zv28DVjOfLyO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScoredSentences:\n",
        "    def __init__(self, sentences, originals, references, max_sentences=None):\n",
        "        self.samples_per_sentence = len(sentences) // len(originals)\n",
        "        self.references_per_sentence = len(references) // len(originals)\n",
        "        self.mbr_consensus_samples_per_sentence = self.samples_per_sentence\n",
        "\n",
        "        if max_sentences is not None:\n",
        "            sentences = sentences[:(max_sentences*self.samples_per_sentence)]\n",
        "            references = references[:(max_sentences*self.references_per_sentence)]\n",
        "            originals = originals[:max_sentences]\n",
        "\n",
        "        self.sentences_flat = sentences\n",
        "        self.originals = originals\n",
        "        self.references = references\n",
        "        self.bleurt = np.array([])\n",
        "        self.comet = np.array([])\n",
        "        self.reranker_score = np.array([])\n",
        "        self.bleu = np.array([])\n",
        "        self.mbr_expected_utility = np.zeros((len(self.originals), self.mbr_consensus_samples_per_sentence))\n",
        "\n",
        "    def compute_scores(self):\n",
        "        references = [self.references[i // self.samples_per_sentence] for i in range(len(self.sentences_flat))]\n",
        "        sources = [self.originals[i // self.samples_per_sentence] for i in range(len(self.sentences_flat))]\n",
        "\n",
        "        # BLEURT\n",
        "        print(\"Computing BLEURT\")\n",
        "        with Timer():\n",
        "            bleurt_flat = bleurt.compute(predictions=self.sentences_flat, references=references)[\"scores\"]\n",
        "            self.bleurt = np.array(bleurt_flat).reshape((len(self.originals), self.samples_per_sentence))\n",
        "\n",
        "        # COMET\n",
        "        print(\"Computing COMET\")\n",
        "        with Timer():\n",
        "            comet_flat = comet_metric.compute(predictions=self.sentences_flat, references=references, sources=sources)[\"scores\"]\n",
        "            self.comet = np.array(comet_flat).reshape((len(self.originals), self.samples_per_sentence))\n",
        "\n",
        "        # BLEU\n",
        "        print(\"Computing BLEU\")\n",
        "        with Timer():\n",
        "            bleu_flat = []\n",
        "            for reference, sentence in zip(references, self.sentences_flat):\n",
        "                bleu_flat.append(sentence_bleu(references=[reference], hypothesis=sentence,\n",
        "                        # smoothing_function=smoothie\n",
        "                  ))\n",
        "            self.bleu = np.array(bleu_flat).reshape((len(self.originals), self.samples_per_sentence))\n",
        "\n",
        "        # Rerank score\n",
        "        print(\"Computing reranker score\")\n",
        "        with Timer():\n",
        "            score_flat = compute_reranker_scores(self.originals, self.sentences_flat)\n",
        "            self.reranker_score = np.array(score_flat).reshape((len(self.originals), self.samples_per_sentence))\n",
        "\n",
        "    def compute_mbr_expected_utility(self):\n",
        "        self.mbr_expected_utility = np.zeros((len(self.originals), self.mbr_consensus_samples_per_sentence))\n",
        "        for i in tqdm(range(len(self.originals))):\n",
        "            refs = []\n",
        "            hyps = []\n",
        "            for j in range(self.mbr_consensus_samples_per_sentence):\n",
        "                hyp_sent = self.sentences_flat[i*self.samples_per_sentence+j]\n",
        "                for k in range(self.mbr_consensus_samples_per_sentence):\n",
        "                    ref_sent = self.sentences_flat[i*self.samples_per_sentence+k]\n",
        "                    refs.append(ref_sent)\n",
        "                    hyps.append(hyp_sent)\n",
        "\n",
        "            bleurt_i = bleurt.compute(predictions=hyps, references=refs)[\"scores\"]\n",
        "            bleurt_i = np.array(bleurt_i).reshape((self.mbr_consensus_samples_per_sentence, self.mbr_consensus_samples_per_sentence))\n",
        "            self.mbr_expected_utility[i] = bleurt_i.mean(axis=-1)\n",
        "\n",
        "\n",
        "    def print_reranker_metrics(self):\n",
        "        print(\"Reranker MSE   \", np.mean((self.reranker_score.reshape((-1,)) - self.bleurt.reshape((-1,)))**2))\n",
        "        print(\"Reranker 1-corr\", 1-np.corrcoef((self.reranker_score.reshape((-1,)), self.bleurt.reshape((-1,))))[0,1])\n",
        "        if len(self.mbr_expected_utility) > 0:\n",
        "          print(\"MBR.E.U. MSE   \", np.mean((self.mbr_expected_utility.reshape((-1,)) - self.bleurt.reshape((-1,)))**2))\n",
        "          print(\"MBR.E.U. 1-corr\", 1-np.corrcoef((self.mbr_expected_utility.reshape((-1,)), self.bleurt.reshape((-1,))))[0,1])\n",
        "        print()\n",
        "\n",
        "def read_from_file(filepath):\n",
        "    result = []\n",
        "    with open(f\"{FULL_DATA_DIR}/{filepath}\", 'r', encoding='utf-8') as fp:\n",
        "        for line in fp:\n",
        "            result.append(line.strip())\n",
        "    return result\n",
        "\n",
        "# Load the data\n",
        "\n",
        "split = \"test\"\n",
        "assert split in [\"dev\", \"test\"]\n",
        "\n",
        "originals = read_from_file(f\"{split}.deu\")\n",
        "references = read_from_file(f\"{split}.eng\")\n",
        "\n",
        "if config[\"compute_mbr_consensus\"]:\n",
        "    max_sentences = len(originals) // 10\n",
        "else:\n",
        "    max_sentences = None\n",
        "\n",
        "generated = ScoredSentences(read_from_file(f\"sampled/{split}.eng\"), originals, references, max_sentences=max_sentences)\n",
        "generated_cold = ScoredSentences(read_from_file(f\"sampled/{split}-cold.eng\"), originals, references, max_sentences=max_sentences)\n",
        "beamsearch = ScoredSentences(read_from_file(f\"beams/{split}.eng\"), originals, references, max_sentences=max_sentences)\n",
        "\n",
        "mult = generated.samples_per_sentence"
      ],
      "metadata": {
        "id": "b2oeD-6sBO1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Score computation"
      ],
      "metadata": {
        "id": "_tQ82_WwfOPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generated.compute_scores()\n",
        "generated_cold.compute_scores()\n",
        "beamsearch.compute_scores()"
      ],
      "metadata": {
        "id": "Mg74qhRfp_cm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if config[\"compute_mbr_consensus\"]:\n",
        "    generated.compute_mbr_expected_utility()\n",
        "    generated_cold.compute_mbr_expected_utility()"
      ],
      "metadata": {
        "id": "kn0_9_PaXz25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated.print_reranker_metrics()\n",
        "generated_cold.print_reranker_metrics()\n",
        "beamsearch.print_reranker_metrics()"
      ],
      "metadata": {
        "id": "gYDaIHBqo7lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples of selected translations"
      ],
      "metadata": {
        "id": "PJY9hnKFfTsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_samples(samples):\n",
        "    for i, (oracle_idx, best_idx, worst_idx) in enumerate(zip(np.argmax(samples.bleurt, axis=-1), np.argmax(samples.reranker_score, axis=-1), np.argmin(samples.reranker_score, axis=-1))):\n",
        "        print(\"Original: \", originals[i])\n",
        "        print(\"Reference:\", references[i])\n",
        "        print(\"Beam srch:\", beamsearch.sentences_flat[i])\n",
        "        print()\n",
        "        print(\"Oracle:   \", samples.sentences_flat[i*mult+oracle_idx])\n",
        "        print(\"Best:     \", samples.sentences_flat[i*mult+best_idx])\n",
        "        print(\"Worst:    \", samples.sentences_flat[i*mult+worst_idx])\n",
        "        print()\n",
        "        beam_score = beamsearch.bleurt[i,0]\n",
        "        oracle_score = samples.bleurt[i,oracle_idx]\n",
        "        best_score = samples.bleurt[i,best_idx]\n",
        "        print(f\"beam: {beam_score:.3f}, best: {best_score:.3f} <= {oracle_score:.3f} (predicted {samples.reranker_score[i][best_idx]:.3f} >= {samples.reranker_score[i][oracle_idx]:.3f})\")\n",
        "        print()\n",
        "        print(30*\"-\")\n",
        "        print()"
      ],
      "metadata": {
        "id": "UzJ-Z4F8P1Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_samples(generated)"
      ],
      "metadata": {
        "id": "uyYF57wnGe8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_samples(generated_cold)"
      ],
      "metadata": {
        "id": "WQ3XwmjGGhTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main evaluation"
      ],
      "metadata": {
        "id": "qI05FPdkfdMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_metrics(samples, display_mbr_consensus=config[\"compute_mbr_consensus\"]):\n",
        "\n",
        "    best_e_oracle = 0\n",
        "    best_e_mbr = 0\n",
        "    mbr_e_oracle = 0\n",
        "\n",
        "    for i, (oracle_idx, best_idx, mbr_idx) in enumerate(zip(np.argmax(samples.bleurt, axis=-1), np.argmax(samples.reranker_score, axis=-1), np.argmax(samples.mbr_expected_utility, axis=-1))):\n",
        "        if oracle_idx == best_idx:\n",
        "            best_e_oracle += 1\n",
        "        if mbr_idx == best_idx:\n",
        "            best_e_mbr += 1\n",
        "        if mbr_idx == oracle_idx:\n",
        "            mbr_e_oracle += 1\n",
        "\n",
        "    print(f\"Our selection is the MBR oracle   on {100*best_e_oracle/len(samples.originals):.02f}% of cases\")\n",
        "    if display_mbr_consensus:\n",
        "        print(f\"Our selection is the MBR cons.    on {100*best_e_mbr/len(samples.originals):.02f}% of cases\")\n",
        "        print(f\"The MBR cons. is the MBR oracle   on {100*mbr_e_oracle/len(samples.originals):.02f}% of cases\")\n",
        "    print()\n",
        "\n",
        "    scores = [(\"bleurt\", samples.bleurt, beamsearch.bleurt), (\"comet\", samples.comet, beamsearch.comet), (\"bleu\", samples.bleu, beamsearch.bleu)]\n",
        "\n",
        "    for (score_name, samples_score, beamsearch_score) in scores:\n",
        "\n",
        "        oracle_ge_beam = 0\n",
        "        best_ge_beam = 0\n",
        "        mbr_ge_beam = 0\n",
        "        best_ge_mbr = 0\n",
        "\n",
        "        beam_sum = 0\n",
        "        oracle_sum = 0\n",
        "        best_sum = 0\n",
        "        mbr_sum = 0\n",
        "\n",
        "\n",
        "        for i, (oracle_idx, best_idx, mbr_idx) in enumerate(zip(np.argmax(samples.bleurt, axis=-1), np.argmax(samples.reranker_score, axis=-1), np.argmax(samples.mbr_expected_utility, axis=-1))):\n",
        "            beam_score = beamsearch_score[i,0]\n",
        "            oracle_score = samples_score[i,oracle_idx]\n",
        "            best_score = samples_score[i,best_idx]\n",
        "            mbr_score = samples_score[i,mbr_idx]\n",
        "\n",
        "            if oracle_score >= beam_score:\n",
        "                oracle_ge_beam += 1\n",
        "            if best_score >= beam_score:\n",
        "                best_ge_beam += 1\n",
        "            if mbr_score >= beam_score:\n",
        "                mbr_ge_beam += 1\n",
        "            if best_score >= mbr_score:\n",
        "                best_ge_mbr += 1\n",
        "\n",
        "            beam_sum += beam_score\n",
        "            oracle_sum += oracle_score\n",
        "            best_sum += best_score\n",
        "            mbr_sum += mbr_score\n",
        "\n",
        "        print(f\"===[{score_name}]===\")\n",
        "        print()\n",
        "        print(f\"The MBR oracle is better than beam on {100*oracle_ge_beam/len(samples.originals):.02f}% of cases\")\n",
        "        print(f\"Our selection  is better than beam on {100*best_ge_beam/len(samples.originals):.02f}% of cases\")\n",
        "        if display_mbr_consensus:\n",
        "            print(f\"The MBR cons.  is better than beam on {100*mbr_ge_beam/len(samples.originals):.02f}% of cases\")\n",
        "            print(f\"Our selection  is better than MBR  on {100*best_ge_mbr/len(samples.originals):.02f}% of cases\")\n",
        "        print(f\"Beam       average score = {100*beam_sum/len(samples.originals):.02f}\")\n",
        "        print(f\"MBR oracle average score = {100*oracle_sum/len(samples.originals):.02f}\")\n",
        "        print(f\"Our selec. average score = {100*best_sum/len(samples.originals):.02f}\")\n",
        "        if display_mbr_consensus:\n",
        "            print(f\"MBR cons.  average score = {100*mbr_sum/len(samples.originals):.02f}\")\n",
        "        print()\n",
        "        print()"
      ],
      "metadata": {
        "id": "ozm_zE93arK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\\n temp = 1\\n\\n\\n\")\n",
        "print_metrics(generated)\n",
        "print(\"\\n\\n\\n temp = 0.7\\n\\n\\n\")\n",
        "print_metrics(generated_cold)"
      ],
      "metadata": {
        "id": "EuSG9k91bSsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation with different hypothesis sample sizes"
      ],
      "metadata": {
        "id": "vnVdWIRuf1P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate how our translations (but not the standard MBR ones) compare for different number of samples"
      ],
      "metadata": {
        "id": "nP9jSIpslz2g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def subsample(samples, num_samples):\n",
        "    num_sentences = len(samples.originals)\n",
        "    num_samples = min(num_samples, samples.samples_per_sentence)\n",
        "    sentences_not_flat = [[samples.sentences_flat[i*samples.samples_per_sentence+j] for j in range(samples.samples_per_sentence)] for i in range(num_sentences)]\n",
        "    sentences_flat = sum([list(x[:num_samples]) for x in sentences_not_flat], [])\n",
        "    result = ScoredSentences(sentences_flat, samples.originals, samples.references)\n",
        "    result.bleurt = samples.bleurt[:, :num_samples]\n",
        "    result.bleu = samples.bleu[:, :num_samples]\n",
        "    result.comet = samples.comet[:, :num_samples]\n",
        "    result.reranker_score = samples.reranker_score[:, :num_samples]\n",
        "    result.mbr_expected_utility = samples.mbr_expected_utility[:, :num_samples]\n",
        "    return result"
      ],
      "metadata": {
        "id": "qBM46sA6HOgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for S in [1, 5, 10]:\n",
        "    print(f\"\\n\\n\\n\\nS = {S}\\n\\n\\n\\n\")\n",
        "\n",
        "    print(\"\\n\\n\\n temp = 1\\n\\n\\n\")\n",
        "    g = subsample(generated, S)\n",
        "    g.compute_scores()\n",
        "    if config[\"compute_mbr_consensus\"]:\n",
        "        g.compute_mbr_expected_utility()\n",
        "    print_metrics(g)\n",
        "\n",
        "    print(\"\\n\\n\\n temp = 0.7\\n\\n\\n\")\n",
        "    g = subsample(generated_cold, S)\n",
        "    g.compute_scores()\n",
        "    if config[\"compute_mbr_consensus\"]:\n",
        "        g.compute_mbr_expected_utility()\n",
        "    print_metrics(g)"
      ],
      "metadata": {
        "id": "YcZQlUSXf-aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End\n",
        "\n",
        "If in Google Colab, kill the session:"
      ],
      "metadata": {
        "id": "dMMVEz_jgXSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "    import time\n",
        "    time.sleep(15)\n",
        "\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ],
      "metadata": {
        "id": "1COCYTqOKqA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeaZnjL2Kyta"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}