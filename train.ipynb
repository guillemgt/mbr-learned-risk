{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Reranker model training"
      ],
      "metadata": {
        "id": "dGBVokvOeByR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up"
      ],
      "metadata": {
        "id": "intnTcPMd6Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_wandb = True"
      ],
      "metadata": {
        "id": "SZI7_THud9M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = \"data\"\n",
        "MODEL_DIR = \"models\""
      ],
      "metadata": {
        "id": "AOHHEK0ENzlM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    FULL_DATA_DIR = f'/content/drive/My Drive/mbr-reranking/{DATA_DIR}'\n",
        "    FULL_MODEL_DIR = f'/content/drive/My Drive/mbr-reranking/{MODEL_DIR}'\n",
        "\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    FULL_DATA_DIR = DATA_DIR\n",
        "    FULL_MODEL_DIR = MODEL_DIR\n",
        "\n",
        "    IN_COLAB = False"
      ],
      "metadata": {
        "id": "5fMdQneGOKyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import sentencepiece\n",
        "except:\n",
        "    !pip install sentencepiece\n",
        "    import sentencepiece"
      ],
      "metadata": {
        "id": "onh3v49PQcp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if use_wandb:\n",
        "    try:\n",
        "        import wandb\n",
        "    except:\n",
        "        !pip install wandb\n",
        "        import wandb"
      ],
      "metadata": {
        "id": "SW1m1JHJONRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "xXTeQzJ7OhFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model config"
      ],
      "metadata": {
        "id": "vgEohUr-d_Jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"head_learning_rate\": 1e-4,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"lr_end_factor\": 1e-2,\n",
        "    \"epochs\": 3,\n",
        "    \"warmup_epochs\": 1,\n",
        "    \"batch_size\": 8,\n",
        "    \"warmup_batch_size\": 32,\n",
        "    \"valid_batch_size\": 128,\n",
        "    \"validation_size\": 16_000,\n",
        "}"
      ],
      "metadata": {
        "id": "ly-c8CFlQ9FV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading dataset (pt. 1)"
      ],
      "metadata": {
        "id": "aOqkRk_ieQTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load the data\n",
        "with open(f\"{FULL_DATA_DIR}/sampled/train.scores\", 'rb') as f:\n",
        "    scores_all = np.load(f)\n",
        "\n",
        "# ref_all = []\n",
        "original_all = []\n",
        "generated_all = []\n",
        "\n",
        "# with open(f\"{FULL_DATA_DIR}/train.eng\", 'r') as fp:\n",
        "#     for line in fp:\n",
        "#         ref_all.append(line.strip())\n",
        "\n",
        "with open(f\"{FULL_DATA_DIR}/train.deu\", 'r') as fp:\n",
        "    for line in fp:\n",
        "        original_all.append(line.strip())\n",
        "\n",
        "with open(f\"{FULL_DATA_DIR}/sampled/train.eng\", 'r') as fp:\n",
        "    for line in fp:\n",
        "        generated_all.append(line.strip())\n",
        "\n",
        "with open(f\"{FULL_DATA_DIR}/sampled/train.info.json\", 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "    samples_per_sentence = metadata[\"samples_per_sentence\"]\n",
        "\n",
        "    original_all = [original_all[i // samples_per_sentence] for i in range(len(generated_all))]\n",
        "    # ref_all = [ref_all[i // samples_per_sentence] for i in range(len(generated_all))]\n",
        "\n",
        "assert(len(original_all) == len(generated_all) == len(scores_all))\n",
        "\n",
        "config[\"train_size\"] = len(original_all) - config[\"validation_size\"]\n",
        "\n",
        "# Train test-split\n",
        "scores_train = scores_all[:-config[\"validation_size\"]]\n",
        "original_train = original_all[:-config[\"validation_size\"]]\n",
        "generated_train = generated_all[:-config[\"validation_size\"]]\n",
        "\n",
        "scores_valid = scores_all[-config[\"validation_size\"]:]\n",
        "original_valid = original_all[-config[\"validation_size\"]:]\n",
        "generated_valid = generated_all[-config[\"validation_size\"]:]"
      ],
      "metadata": {
        "id": "8bBEWgrwePTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "uvWWk14IeHPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean pooling\n",
        "class MeanPooling(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        # Mean pooling\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(hidden_states * input_mask_expanded, 1)\n",
        "        sum_mask = input_mask_expanded.sum(1)\n",
        "        mean_pooled = sum_embeddings / sum_mask\n",
        "        return mean_pooled\n",
        "\n",
        "# Model\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, pretrained_model):\n",
        "        super().__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.regression_head = torch.nn.Linear(pretrained_model.config.hidden_size, 1)\n",
        "        self.pooling = MeanPooling()\n",
        "        self.pretrained_frozen = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        if self.pretrained_frozen:\n",
        "            with torch.no_grad():\n",
        "                token_embeddings = self.pretrained_model(input_ids, attention_mask=attention_mask)\n",
        "                pooled_embedding = self.pooling(token_embeddings.last_hidden_state, attention_mask)\n",
        "        else:\n",
        "            token_embeddings = self.pretrained_model(input_ids, attention_mask=attention_mask)\n",
        "            pooled_embedding = self.pooling(token_embeddings.last_hidden_state, attention_mask)\n",
        "        return self.regression_head(pooled_embedding)\n",
        "\n",
        "    def freeze_pretrained(self):\n",
        "        self.pretrained_frozen = True\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_pretrained(self):\n",
        "        self.pretrained_frozen = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = True"
      ],
      "metadata": {
        "id": "yNYV5pLXbZv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "pretrained_model = XLMRobertaModel.from_pretrained('xlm-roberta-base')\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "# Define the model\n",
        "model = RegressionModel(pretrained_model)\n",
        "\n",
        "# Optimizer settings\n",
        "head_optimizer = AdamW(model.regression_head.parameters(), lr=config[\"head_learning_rate\"])\n",
        "optimizer = AdamW(model.parameters(), lr=config[\"learning_rate\"])\n",
        "scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=config[\"lr_end_factor\"], total_iters=(config[\"epochs\"]-config[\"warmup_epochs\"]) * len(generated_train) // config[\"batch_size\"])\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "model.train()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "Qtlv6MMk4JFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset (pt. 2)"
      ],
      "metadata": {
        "id": "iqC2Ae9UeWd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Sampler\n",
        "import random\n",
        "import math\n",
        "\n",
        "class SimilarLengthSampler(Sampler):\n",
        "    def __init__(self, dataset, batch_size, shuffle=False):\n",
        "        super().__init__(dataset)\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "        # Group sorted indices into bins of size 'batch_size * 100' (or any large number)\n",
        "        bin_size = batch_size * 100\n",
        "        self.bins = [list(range(i, min(i + bin_size, len(dataset)))) for i in range(0, len(dataset), bin_size)]\n",
        "\n",
        "    def __iter__(self):\n",
        "        if self.shuffle:\n",
        "            # Shuffle bins and sequences within each bin\n",
        "            random.shuffle(self.bins)\n",
        "            bins = [random.sample(bin, len(bin)) for bin in self.bins]\n",
        "        else:\n",
        "            bins = self.bins\n",
        "\n",
        "        # Flatten the list of bins and chunk them into batches\n",
        "        flattened_bins = [idx for bin in bins for idx in bin]\n",
        "        batches = [flattened_bins[i:i + self.batch_size] for i in range(0, len(flattened_bins), self.batch_size)]\n",
        "\n",
        "        if self.shuffle:\n",
        "            # Optionally shuffle the batches\n",
        "            random.shuffle(batches)\n",
        "\n",
        "        for batch in batches:\n",
        "            yield batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return math.ceil(len(self.dataset) / self.batch_size)"
      ],
      "metadata": {
        "id": "fIkAeYqngcq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, original, generated, targets):\n",
        "        texts = [o + tokenizer.sep_token + g for g, o in zip(generated, original)]\n",
        "        self.encodings = tokenizer(texts, truncation=True, padding=True)\n",
        "        self.targets = torch.tensor(targets, dtype=torch.float)\n",
        "\n",
        "        self.sorted_indices = sorted(range(len(texts)), key=lambda i: sum(self.encodings[\"attention_mask\"][i]))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        idx = self.sorted_indices[index]\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.targets[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.targets)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "dataset_train = RegressionDataset(original_train, generated_train, scores_train)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "dataloader_train_warmup = DataLoader(dataset_train, batch_size=config[\"warmup_batch_size\"], shuffle=True)\n",
        "\n",
        "dataset_valid = RegressionDataset(original_valid, generated_valid, scores_valid)\n",
        "dataloader_valid = DataLoader(dataset_valid, batch_size=config[\"valid_batch_size\"], shuffle=True)"
      ],
      "metadata": {
        "id": "8mXqnCMbPiuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def truncate_batch(input_ids, attention_mask):\n",
        "    # Find the maximum sequence length in this batch\n",
        "    max_len = attention_mask.sum(dim=1).max().item()\n",
        "\n",
        "    # Truncate input_ids and attention_mask to max_len\n",
        "    truncated_input_ids = input_ids[:, :max_len]\n",
        "    truncated_attention_mask = attention_mask[:, :max_len]\n",
        "\n",
        "    return truncated_input_ids, truncated_attention_mask\n",
        "\n",
        "# @torch.compile\n",
        "def train_batch(batch, opt):\n",
        "    input_ids = batch['input_ids']\n",
        "    attention_mask = batch['attention_mask']\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    input_ids, attention_mask = truncate_batch(input_ids, attention_mask)\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "        outputs = model(input_ids, attention_mask=attention_mask).squeeze()\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(opt)\n",
        "    scaler.update()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    return loss.detach().item()"
      ],
      "metadata": {
        "id": "x3-jH15KhdSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "ZsIngUdFec9H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eVDm-dnNMIw"
      },
      "outputs": [],
      "source": [
        "if use_wandb:\n",
        "    wandb.init(project=\"mbr-reranking\", config=config)\n",
        "    wandb_run_name = wandb.run.name\n",
        "\n",
        "model_name = \"model\"\n",
        "if use_wandb:\n",
        "    model_name = f\"{wandb_run_name}\"\n",
        "\n",
        "update_loss_every = 10\n",
        "wandb_logs_per_epoch = 100\n",
        "\n",
        "wandb_steps_per_log = (len(dataloader_train) + wandb_logs_per_epoch-1) // wandb_logs_per_epoch\n",
        "\n",
        "random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "running_loss = 0\n",
        "running_loss_correction = 0\n",
        "ema_factor = 0.002\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# Training loop\n",
        "model.freeze_pretrained()\n",
        "opt = head_optimizer\n",
        "dataloader = dataloader_train_warmup\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "\n",
        "    # Freeze the pre-trained model's parameters in the first epoch\n",
        "    if epoch >= config[\"warmup_epochs\"]:\n",
        "        model.unfreeze_pretrained()\n",
        "        opt = optimizer\n",
        "        dataloader = dataloader_train\n",
        "\n",
        "    loss_sum = 0\n",
        "    loss_items = 0\n",
        "\n",
        "    # Train\n",
        "    pbar = tqdm(dataloader, total=len(dataloader))\n",
        "    for step, batch in enumerate(pbar):\n",
        "        loss = train_batch(batch, opt)\n",
        "\n",
        "        loss_sum += loss\n",
        "        loss_items += 1\n",
        "\n",
        "        running_loss = (1-ema_factor)*running_loss + ema_factor*loss\n",
        "        running_loss_correction = (1-ema_factor)*running_loss_correction + ema_factor\n",
        "\n",
        "        if step % update_loss_every == update_loss_every-1:\n",
        "            pbar.set_description(f\"Train loss: {running_loss / running_loss_correction:.3f}\")\n",
        "\n",
        "        if step % wandb_steps_per_log == wandb_steps_per_log-1:\n",
        "            # Log metrics to wandb\n",
        "            if use_wandb:\n",
        "                wandb.log({\"epoch\": epoch + step / len(dataloader), \"running_loss\": running_loss / running_loss_correction, \"lr\": scheduler.get_last_lr()[0]})\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if epoch >= config[\"warmup_epochs\"]:\n",
        "            scheduler.step()\n",
        "\n",
        "    # Validate\n",
        "    valid_loss_sum = 0\n",
        "    valid_loss_items = 0\n",
        "    pbar = tqdm(dataloader_valid, total=len(dataloader_valid))\n",
        "    for step, batch in enumerate(pbar):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with torch.autocast(device_type=device, dtype=torch.float16):\n",
        "                outputs = model(input_ids, attention_mask=attention_mask).squeeze()\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "        valid_loss_sum += loss\n",
        "        valid_loss_items += 1\n",
        "\n",
        "        if step % update_loss_every == update_loss_every-1:\n",
        "            pbar.set_description(f\"Valid loss: {valid_loss_sum / valid_loss_items:.3f}\")\n",
        "\n",
        "\n",
        "    # Log metrics to wandb\n",
        "    if use_wandb:\n",
        "        wandb.log({\"epoch\": epoch+1, \"train_loss\": loss_sum / loss_items, \"valid_loss\": valid_loss_sum / valid_loss_items})\n",
        "\n",
        "    # Save the model if it's better\n",
        "    if valid_loss_sum / valid_loss_items < best_valid_loss:\n",
        "        print(\"Saving model\")\n",
        "        torch.save(model.state_dict(), f'{FULL_MODEL_DIR}/{model_name}.pt')\n",
        "        best_valid_loss = valid_loss_sum / valid_loss_items\n",
        "\n",
        "    print(f\"Epoch: {epoch}, Train loss: {loss_sum / loss_items}, Valid loss: {valid_loss_sum / valid_loss_items}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finalize wandb run\n",
        "if use_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "lpbvdRWDV5to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## End\n",
        "\n",
        "If in Google Colab, kill the session:"
      ],
      "metadata": {
        "id": "Up4nqeC5ekSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if IN_COLAB:\n",
        "    import time\n",
        "    time.sleep(15)\n",
        "\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()"
      ],
      "metadata": {
        "id": "CYjEqiKti9oH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EpQSTQqnKCA4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}